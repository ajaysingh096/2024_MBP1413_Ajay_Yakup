{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKMMrpaufMlr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import json\n",
        "\n",
        "# You must add ubteacher and detectron2 to your environment path to run the training\n",
        "# ubteacher is provided in this repo\n",
        "# detectron2 can be installed by following this tutorial: https://detectron2.readthedocs.io/en/latest/tutorials/install.html\n",
        "\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import default_argument_parser, default_setup, launch\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "\n",
        "from ubteacher import add_ubteacher_config\n",
        "from ubteacher.engine.trainer import UBTeacherTrainer, UBRCNNTeacherTrainer\n",
        "from ubteacher.modeling import EnsembleTSModel\n"
      ],
      "metadata": {
        "id": "b7OdBpLNmIGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract images and combine annotations\n",
        "\n",
        "In this dataset, the images are nested in folders, with each folder containing images and masks. Each object has its own mask, so its necessary that we generate annotations from each mask image individually before merging them into an annotation. This notebook will prepare the data for training by generating a detectron2-compatible json file containing all the training and validation images and their corresponding annotations."
      ],
      "metadata": {
        "id": "2UAdhal8nHWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WARNING: This cell will output thousands of files\n",
        "\n",
        "\n",
        "# quick class because json serialization doesn't like numpy arrays\n",
        "class NpEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        return super(NpEncoder, self).default(obj)\n",
        "\n",
        "# establish folder structure\n",
        "parent_dir = '/home/chao_lab/SynologyDrive/chaolab_AI_path/ajay_mbp1413'\n",
        "train_dir = os.path.join(parent_dir, 'train')\n",
        "out_dir = os.path.join(parent_dir, 'train_images')\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "\n",
        "# detectron2 format {train: [{d2 compatible dict}, ...], val: [{d2 compatible dict}, ...]}\n",
        "train_dicts = []\n",
        "val_dicts = []\n",
        "\n",
        "for f in os.listdir(train_dir):\n",
        "    img_dir = os.path.join(train_dir, f, 'images')\n",
        "    img_name = os.listdir(img_dir)[0]\n",
        "    img_noext = os.path.splitext(img_name)[0]\n",
        "    img_npy = os.path.join(out_dir, img_noext + '.npy')\n",
        "    mask_dir = os.path.join(train_dir, f, 'masks')\n",
        "    img_path = os.path.join(img_dir, img_name)\n",
        "    img = cv2.imread(img_path)\n",
        "    # save as numpy in out_dir\n",
        "    out_path = os.path.join(out_dir, img_noext)\n",
        "    img_array = np.array(img)\n",
        "    #np.save(out_path, img) - commented out in case someone accidentally runs this\n",
        "    all_dicts = []\n",
        "    # get each mask and generate annotation\n",
        "    for mask in os.listdir(mask_dir):\n",
        "        mask_path = os.path.join(mask_dir, mask)\n",
        "        mask_img = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask_img = mask_img / 255\n",
        "        mask_img = mask_img.astype(np.uint8)\n",
        "        # convert to polygon\n",
        "        contours, _ = cv2.findContours(mask_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        # for each contour, create a mask annotation\n",
        "        for contour in contours:\n",
        "            # get bbox\n",
        "            x1, y1, w, h = cv2.boundingRect(contour)\n",
        "            x2, y2 = x1 + w, y1 + h\n",
        "            coords = []\n",
        "            for point in contour:\n",
        "                coords.append(point[0].astype(int))\n",
        "            # cv2 contours are not closed, so we close them\n",
        "            coords.append(contour[0][0].astype(int))\n",
        "            coords = np.array(coords)\n",
        "            coords = coords.flatten()\n",
        "            new_coords = []\n",
        "            for i in coords:\n",
        "                new_coords.append(i)\n",
        "            # for each object, a dict is required\n",
        "            each_dict = {\"category_id\": 0, # only one class to be detected\n",
        "                        \"bbox\": [x1, y1, x2, y2],\n",
        "                        \"bbox_mode\": 0, # bbox mode xywh\n",
        "                        \"segmentation\": [new_coords]} # segmentation for mask head\n",
        "        all_dicts.append(each_dict)\n",
        "    # for each image, a dict containing all object dicts is required\n",
        "    total_dict = {\"file_name\": img_npy,\n",
        "                  \"image_id\": img_noext,\n",
        "                  \"height\": img.shape[0],\n",
        "                  \"width\": img.shape[1],\n",
        "                  \"annotations\": all_dicts} # all objects in the image\n",
        "    # randomly choose if anno will be in train or validation set -- 70/30 split\n",
        "    if random.random() > 0.3:\n",
        "        train_dicts.append(total_dict)\n",
        "    else:\n",
        "        val_dicts.append(total_dict)\n",
        "\n",
        "# we generate the final format and save it as a json file\n",
        "final_dict = {\"train\": train_dicts, \"val\": val_dicts}\n",
        "with open(os.path.join(parent_dir, 'annotations.json'), 'w') as f:\n",
        "    json.dump(final_dict, f, indent = 4, cls=NpEncoder) # indenting helps legibility\n",
        "\n",
        "# Now you will have a json with all of the annotations saved to your parent_dir, as well as every image in your training set as a numpy array"
      ],
      "metadata": {
        "id": "9bkwl6LYL4ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Pre-processing\n",
        "\n",
        "We use normalization only to match with Unet results"
      ],
      "metadata": {
        "id": "7Ehxue5vMPMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4iCfJ7u5kwe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data pre-processing steps for fluorescence microscopy images and brightfield H&E images, along with their masks:\n",
        "\n",
        "def normalization(img):\n",
        "    # Generate mask by thresholding and morphological transformations\n",
        "    kernel = (3, 3)\n",
        "    # Step 1: Normalization\n",
        "     # mitigates varying lighting conditions and provides a consistent range of values\n",
        "    normalized = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    return normalized\n",
        "\n",
        "# save preprocessed to disk\n",
        "\n",
        "in_dir = '/home/chao_lab/SynologyDrive/chaolab_AI_path/ajay_mbp1413/train_images'\n",
        "out_dir = '/home/chao_lab/SynologyDrive/chaolab_AI_path/ajay_mbp1413/train_images_preprocessed'\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "for i in glob.glob(in_dir + '/*.npy'):\n",
        "    img = np.load(i)\n",
        "    img_name = os.path.basename(i)\n",
        "    normalized_img = normalization(i)\n",
        "    np.save(os.path.join(out_dir, img_name), normalized_img)"
      ],
      "metadata": {
        "id": "f_lso9JLwyUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n",
        "\n",
        "This code is an edited version of train_net.py from the unbiasedteacher repo.\n",
        "https://github.com/facebookresearch/unbiased-teacher\n",
        "\n",
        "We train a Mask-RCNN model based on the unbiasedteacher repo. Unfortunately we didn't have the resources to create a semi-supervised model, which was our original plan. However, we can still use the detectron2 features in this repo, including dataset registration, config, logging and train-time validation, which helps us organize training and tune hyperparameters. We first import our config which contains important paths for I/O, controls training parameters, types of ROI heads (i.e. inclusion of masks) and much more. Then, we split and register our dataset based on our annotation json. We establish that we are using the RCNN training engine. Because we originally intended to have a semi-supervised model, the student/teacher models are still assembled into an ensemble, however only the student model will be trained (this is native to ubteacher, which allows fully supervised training despite loading an ensemble model). Finally, the model is trained!"
      ],
      "metadata": {
        "id": "DWecmsmhnq7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
        "\n",
        "# Duplicate of train_maskrcnn.py in the repo\n",
        "\n",
        "# Create some basic util functions for splitting and registering\n",
        "\n",
        "def split_dataset(cfg):\n",
        "    \"\"\"Function to split a dataset into 'train' and 'val' sets.\n",
        "    Args:\n",
        "    dataset_dicts: a list of dicts in detectron2 dataset format\n",
        "    \"\"\"\n",
        "    with open(cfg.DATASET_DICTS, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        train_set = data['train']\n",
        "        val_set = data['val']\n",
        "        return train_set, val_set\n",
        "\n",
        "def register_dataset(dset_type, dataset_dicts):\n",
        "        \"\"\"Helper function to register a new dataset to detectron2's\n",
        "        Datasetcatalog and Metadatacatalog.\n",
        "\n",
        "        Args:\n",
        "        dataset_dicts -- list of dicts in detectron2 dataset format\n",
        "        cat_map -- dictionary to map categories to ids, e.g. {'ROI':0, 'JUNK':1}\n",
        "        \"\"\"\n",
        "        reg_name = dset_type\n",
        "\n",
        "        # Register dataset to DatasetCatalog\n",
        "        print(f\"working on '{reg_name}'...\")\n",
        "\n",
        "        DatasetCatalog.register(\n",
        "            reg_name,\n",
        "            lambda d=dset_type: dataset_dicts\n",
        "        )\n",
        "        MetadataCatalog.get(reg_name).set(\n",
        "            thing_classes='0',\n",
        "        )\n",
        "\n",
        "        return MetadataCatalog\n",
        "\n",
        "def setup(args):\n",
        "    \"\"\"\n",
        "    Create configs and perform basic setups.\n",
        "    \"\"\"\n",
        "    cfg = get_cfg()\n",
        "    cfg.set_new_allowed(True) #allows custom cfg keys\n",
        "    add_ubteacher_config(cfg)\n",
        "    cfg.merge_from_file(args.config_file)\n",
        "    cfg.merge_from_list(args.opts)\n",
        "    cfg.freeze()\n",
        "    default_setup(cfg, args)\n",
        "    return cfg\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    cfg = setup(args)\n",
        "\n",
        "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # split and register\n",
        "    with open(cfg.DATASET_DICTS, 'r') as f:\n",
        "        train_labeled, val = split_dataset(cfg)\n",
        "        register_dataset(\"train\", train_labeled)\n",
        "        register_dataset(\"val\", val)\n",
        "\n",
        "    # train\n",
        "    if cfg.SEMISUPNET.Trainer == \"ubteacher\":\n",
        "        Trainer = UBTeacherTrainer\n",
        "    elif cfg.SEMISUPNET.Trainer == \"ubteacher_rcnn\":\n",
        "        Trainer = UBRCNNTeacherTrainer\n",
        "\n",
        "    if args.eval_only:\n",
        "        if cfg.SEMISUPNET.Trainer == \"ubteacher\":\n",
        "            model = Trainer.build_model(cfg)\n",
        "            model_teacher = Trainer.build_model(cfg)\n",
        "            ensem_ts_model = EnsembleTSModel(model_teacher, model)\n",
        "\n",
        "            DetectionCheckpointer(\n",
        "                ensem_ts_model, save_dir=cfg.OUTPUT_DIR\n",
        "            ).resume_or_load(cfg.MODEL.WEIGHTS, resume=args.resume)\n",
        "            res = Trainer.test(cfg, ensem_ts_model.modelTeacher)\n",
        "        else:\n",
        "            model = Trainer.build_model(cfg)\n",
        "            DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(\n",
        "                cfg.MODEL.WEIGHTS, resume=args.resume\n",
        "            )\n",
        "            res = Trainer.test(cfg, model)\n",
        "        return res\n",
        "\n",
        "    trainer = Trainer(cfg)\n",
        "    trainer.resume_or_load(resume=args.resume)\n",
        "\n",
        "    return trainer.train()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = default_argument_parser().parse_args()\n",
        "\n",
        "    print(\"Command Line Args:\", args)\n",
        "    launch(\n",
        "        main,\n",
        "        args.num_gpus,\n",
        "        num_machines=args.num_machines,\n",
        "        machine_rank=args.machine_rank,\n",
        "        dist_url=args.dist_url,\n",
        "        args=(args,),\n",
        "    )"
      ],
      "metadata": {
        "id": "vtX0LjQt7KnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Inference and Get DICE Score\n",
        "\n",
        "For inference, we assemble the model as an ensemble despite the fact that the teacher model has not been trained. For our purposes (fully supervised), we will only use the trained student model. The config is initialized, the model weights are loaded from the .pth file and the confidence threshold is set to filter low confidence predictions. We must still create a dictionary for the image even though there is no labels. Finally, the image is converted to a torch-compatible tensor and our Mask-RCNN model undergoes a forward pass to create predictions."
      ],
      "metadata": {
        "id": "JSUlzWsDl4xQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "# out_path = '/home/chao_lab/SynologyDrive/chaolab_AI_path/ajay_mbp1413/visuals'\n",
        "# load model\n",
        "model_path = '/home/chao_lab/SynologyDrive/chaolab_AI_path/ajay_mbp1413/model_preprocess/model_0001999.pth' # replace with your own model path\n",
        "config_path = '/home/chao_lab/SynologyDrive/chaolab_AI_path/ajay_mbp1413/model_preprocess/config.yaml' # replace with your own config path\n",
        "# Load cfg\n",
        "cfg = get_cfg()\n",
        "cfg.set_new_allowed(True)\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = model_path\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "\n",
        "# Assemble ensemble model despite not actually training the teacher\n",
        "student_model = UBRCNNTeacherTrainer.build_model(cfg)\n",
        "teacher_model = UBRCNNTeacherTrainer.build_model(cfg)\n",
        "model = EnsembleTSModel(teacher_model, student_model)\n",
        "model.eval()\n",
        "used_model = model.modelStudent # we only use the student model since it is fully supervised\n",
        "# Load model weights from checkpoint\n",
        "checkpointer = DetectionCheckpointer(model)\n",
        "checkpointer.load(cfg.MODEL.WEIGHTS)\n",
        "\n",
        "# More util functions\n",
        "def get_unlabeled(img_file):\n",
        "\n",
        "    \"\"\"\n",
        "    Get unlabeled image dict for detectron2\n",
        "    \"\"\"\n",
        "    f = np.load(img_file)\n",
        "    shape = f.shape\n",
        "    del f # use del instead of with because numpy version issue\n",
        "    img_base = os.path.basename(os.path.splitext(img_file)[0])\n",
        "    ## Fill remaining fields\n",
        "    dataset_dicts = [{'file_name': img_file,\n",
        "                    'height': shape[0],\n",
        "                    'width': shape[1],\n",
        "                    'image_id': img_base}\n",
        "                    ]\n",
        "    return dataset_dicts\n",
        "\n",
        "def detectron2_visualizer(img, outputs):\n",
        "    \"\"\"\n",
        "    Create visualizations of our model outputs\n",
        "    \"\"\"\n",
        "    v = Visualizer(img[:, :, ::-1], scale=1.2)\n",
        "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    return v.get_image()[:, :, ::-1]\n",
        "\n",
        "def main(img_file, model):\n",
        "# Single-image inference script\n",
        "    dicts = get_unlabeled(img_file) # create dictionary for registration\n",
        "    #print(dicts)\n",
        "    fname = dicts[0]['file_name']\n",
        "    #print(f\"Processing {fname}\")\n",
        "    f_id = fname.split('/')[-1].split('.')[0]\n",
        "\n",
        "    mask_dir = os.path.join(train_dir, f_id, 'masks')\n",
        "    final_gt = np.zeros_like(np.load(fname)[:,:,0])\n",
        "    for mask in os.listdir(mask_dir):\n",
        "        mask_path = os.path.join(mask_dir, mask)\n",
        "        mask_img = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask_img = mask_img / 255\n",
        "        mask_img = mask_img.astype(np.uint8)\n",
        "        final_gt += mask_img\n",
        "\n",
        "    img = np.load(dicts[0]['file_name'])\n",
        "    im = torch.from_numpy(img).permute(2, 0, 1) # convert to torch tensor\n",
        "    inputs = [{\"image\": im, \"height\": im.shape[1], \"width\": im.shape[2]}]\n",
        "    with torch.no_grad():\n",
        "        outputs = used_model(inputs)\n",
        "        instances = outputs[0][\"instances\"].to(\"cpu\")\n",
        "        final_pred = np.zeros_like(img[:,:,0])\n",
        "        for i in range(len(instances)):\n",
        "            pred = instances[i].pred_masks.numpy()[0].tolist()\n",
        "            final_pred += pred\n",
        "\n",
        "        def dice_score(gt, pred):\n",
        "            intersection = np.sum(gt * pred)\n",
        "            union = np.sum(gt) + np.sum(pred)\n",
        "            dice = 2 * intersection / union\n",
        "            return dice\n",
        "\n",
        "    dice_score = dice_score(final_gt, final_pred)\n",
        "\n",
        "    # visualize our outputs\n",
        "    fig, ax = plt.subplots(2, figsize=(20, 20))\n",
        "    ax[0].imshow(img)\n",
        "    ax[1].imshow(detectron2_visualizer(img, outputs[0]))\n",
        "\n",
        "    plt.show()\n",
        "    #fig.savefig(os.path.join(out_path, f_id + '.png'), dpi=800) # commented out since it writes to disk\n",
        "    plt.close()\n",
        "\n",
        "    return dice_score\n",
        "\n",
        "\n",
        "# Can be run on a single image or entire dataset through iteration\n",
        "# Example:\n",
        "#each_ds = []\n",
        "#for i in glob.glob('/home/chao_lab/SynologyDrive/chaolab_AI_path/ajay_mbp1413/val_images/*.npy'): # Replace with your own path\n",
        "#    each_ds.append(main(i, used_model))\n",
        "#final_ds = np.mean(each_ds)\n",
        "#print(final_ds)"
      ],
      "metadata": {
        "id": "Qi34g6VcMyHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unbiased teacher detectron2-format config -- anything that isn't commented is default as per the original ubteacher implementation\n",
        "\n",
        "# This is a duplicate of train_maskrcnn.yaml in the repo\n",
        "\n",
        "_BASE_: \"../Base-RCNN-FPN.yaml\"\n",
        "MODEL:\n",
        "  META_ARCHITECTURE: \"TwoStagePseudoLabGeneralizedRCNN\"\n",
        "  WEIGHTS: \"detectron2://ImageNetPretrained/MSRA/R-50.pkl\" #Inherit ImageNet weights for a ResNet-50. https://github.com/facebookresearch/detectron2/blob/main/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\n",
        "  RESNETS:\n",
        "    DEPTH: 50\n",
        "  MASK_ON: True # Use mask head in ROI heads\n",
        "  PROPOSAL_GENERATOR:\n",
        "    NAME: \"PseudoLabRPN\"\n",
        "  RPN:\n",
        "    POSITIVE_FRACTION: 0.25\n",
        "    LOSS: \"CrossEntropy\"\n",
        "  ROI_HEADS:\n",
        "    NAME: \"MaskROIHeadsPseudoLab\"\n",
        "    LOSS: \"FocalLoss_BoundaryVar\"\n",
        "    NUM_CLASSES: 1 # We are only detecting one class - nuclei\n",
        "  ROI_BOX_HEAD:\n",
        "    BBOX_REG_LOSS_TYPE: \"nlloss\"\n",
        "    CLS_AGNOSTIC_BBOX_REG: true\n",
        "SOLVER:\n",
        "  LR_SCHEDULER_NAME: \"WarmupMultiStepLR\"\n",
        "  IMG_PER_BATCH_LABEL: 8\n",
        "  BASE_LR: 0.005 # Low learning rate prevents overfitting\n",
        "  STEPS: (10000,) # Number of iterations\n",
        "  MAX_ITER: 10000\n",
        "  CHECKPOINT_PERIOD: 2000 # Return metrics every 2000 iters.\n",
        "  AMP:\n",
        "    ENABLED: False\n",
        "DATALOADER:\n",
        "  SUP_PERCENT: 100.0\n",
        "  RANDOM_DATA_SEED: 1\n",
        "  FILTER_EMPTY_ANNOTATIONS: false # Allow for training on unlabeled images\n",
        "DATASETS:\n",
        "  CROSS_DATASET: False\n",
        "  TRAIN: (\"train\",)\n",
        "  TEST: (\"val\",)\n",
        "  # We avoid using semi-supervised because of the difficulties of implementation\n",
        "SEMISUPNET:\n",
        "  Trainer: \"ubteacher_rcnn\"\n",
        "  PSEUDO_BBOX_SAMPLE: \"thresholding\"\n",
        "  PSEUDO_BBOX_SAMPLE_REG: \"thresholding\" # 0.5 when PSEUDO_BBOX_SAMPLE_REG = 'thresholding'\n",
        "  BBOX_THRESHOLD: 0.5\n",
        "  BBOX_THRESHOLD_REG: 0.5 # 0.5 when PSEUDO_BBOX_SAMPLE_REG = 'thresholding'\n",
        "  BBOX_CTR_THRESHOLD: 0.0\n",
        "  BBOX_CTR_THRESHOLD_REG: 0.0\n",
        "  TEACHER_UPDATE_ITER: 1\n",
        "  BURN_UP_STEP: 10000\n",
        "  EMA_KEEP_RATE: 0.9999\n",
        "  UNSUP_LOSS_WEIGHT: 3.0\n",
        "  UNSUP_REG_LOSS_WEIGHT: 0.2\n",
        "  CONSIST_CTR_LOSS: \"pseudo\"\n",
        "  PSEUDO_CLS_IGNORE_NEAR: False\n",
        "  PSEUDO_CTR_THRES: 0.5\n",
        "  SOFT_CLS_LABEL: False\n",
        "  CLS_LOSS_METHOD: \"focal\"\n",
        "  CLS_LOSS_PSEUDO_METHOD: \"focal\"\n",
        "  TS_BETTER: 0.1\n",
        "  CONSIST_REG_LOSS: \"ts_locvar_better_nms_nll_l1\"\n",
        "  ANALYSIS_PRINT_FRE: 5000\n",
        "  ANALYSIS_ACCUMLATE_FRE: 50\n",
        "INPUT:\n",
        "  MIN_SIZE_TRAIN: (500,)\n",
        "  MAX_SIZE_TRAIN: 800\n",
        "TEST:\n",
        "  EVAL_PERIOD: 1000 # Output metrics every 1000 iterations\n",
        "  EVALUATOR: \"COCOeval\"\n",
        "  VAL_LOSS: False # Don't update the model during validation\n",
        "NUMPY: True # Use numpy inputs - custom implementation of ubteacher from Chao Lab\n",
        "DATASET_DICTS: /home/chao_lab/SynologyDrive/chaolab_AI_path/ajay_mbp1413/annotations.json # Path to annotation file\n",
        "IMG_DIR: /home/chao_lab/SynologyDrive/chaolab_AI_path/ajay_mbp1413/train_images_preprocessed # Path to image folder"
      ],
      "metadata": {
        "id": "LCA0Sp6QQ9a7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}